'use server';
/**
 * @fileOverview A multimodal response generation AI agent.
 *
 * - generateMultimodalResponse - A function that handles the multimodal response generation process.
 * - GenerateMultimodalResponseInput - The input type for the generateMultimodalResponse function.
 * - GenerateMultimodalResponseOutput - The return type for the generateMultimodalResponse function.
 */

import { ai } from '@/ai/genkit';
import { z } from 'genkit';
import { googleAI } from '@genkit-ai/googleai';
import wav from 'wav';

// Define input schema
const GenerateMultimodalResponseInputSchema = z.object({
  questionText: z.string().optional().describe('The question asked by the student as text.'),
  questionImage: z
    .string()
    .optional()
    .describe(
      "A photo of a question, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
  questionAudio: z
    .string()
    .optional()
    .describe(
      "An audio recording of a question, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
});
export type GenerateMultimodalResponseInput = z.infer<typeof GenerateMultimodalResponseInputSchema>;

// Define output schema
const GenerateMultimodalResponseOutputSchema = z.object({
  textResponse: z.string().describe('The text part of the answer.'),
  imageResponse: z.string().optional().describe('An image generated by the AI, as a data URI.'),
  audioResponse: z.string().optional().describe('A spoken explanation, as a data URI.'),
});
export type GenerateMultimodalResponseOutput = z.infer<typeof GenerateMultimodalResponseOutputSchema>;

// Exported function to be called from the frontend
export async function generateMultimodalResponse(
  input: GenerateMultimodalResponseInput
): Promise<GenerateMultimodalResponseOutput> {
  return generateMultimodalResponseFlow(input);
}

// Helper to convert PCM audio to WAV format
async function toWav(
  pcmData: Buffer,
  channels = 1,
  rate = 24000,
  sampleWidth = 2
): Promise<string> {
  return new Promise((resolve, reject) => {
    const writer = new wav.Writer({
      channels,
      sampleRate: rate,
      bitDepth: sampleWidth * 8,
    });
    const bufs: any[] = [];
    writer.on('error', reject);
    writer.on('data', d => bufs.push(d));
    writer.on('end', () => resolve(Buffer.concat(bufs).toString('base64')));
    writer.write(pcmData);
    writer.end();
  });
}

// Genkit flow definition
const generateMultimodalResponseFlow = ai.defineFlow(
  {
    name: 'generateMultimodalResponseFlow',
    inputSchema: GenerateMultimodalResponseInputSchema,
    outputSchema: GenerateMultimodalResponseOutputSchema,
  },
  async input => {
    const { questionText, questionImage, questionAudio } = input;

    // Construct the main prompt for text and image generation
    const textPrompt = `You are a friendly and helpful AI tutor for students. Your goal is to provide clear, concise, and engaging explanations.

Analyze the student's question from the provided text, image, or audio.

1.  **Provide a step-by-step text explanation** to answer the question.
2.  **Determine if a visual aid would be helpful.** If the question involves geometry, diagrams, charts, or complex visual concepts, generate a simple, clear image or diagram to illustrate your explanation. Your response should include a placeholder like "[[GENERATE_IMAGE: A diagram of...]]" where the image should be.
3.  **Keep your text response separate from the image generation instruction.**

Question (text): ${questionText || 'N/A'}
{{#if questionImage}}
Question (image): {{media url=questionImage}}
{{/if}}
{{#if questionAudio}}
Question (audio): {{media url=questionAudio}}
{{/if}}
`;

    // 1. Generate the initial text response (and decide if an image is needed)
    const llmResponse = await ai.generate({
      prompt: textPrompt,
      model: googleAI.model('gemini-2.0-flash'),
    });

    let textResponse = llmResponse.text;
    let imageResponse: string | undefined = undefined;

    // 2. Check for image generation request and generate image if needed
    const imageGenRegex = /\[\[GENERATE_IMAGE: (.*?)\]\]/;
    const imageMatch = textResponse.match(imageGenRegex);
    textResponse = textResponse.replace(imageGenRegex, '').trim(); // Clean up the placeholder

    if (imageMatch) {
      const imagePrompt = imageMatch[1];
      const { media } = await ai.generate({
        model: 'googleai/gemini-2.0-flash-preview-image-generation',
        prompt: imagePrompt,
        config: {
          responseModalities: ['TEXT', 'IMAGE'],
        },
      });
      imageResponse = media?.url;
    }

    // 3. Generate text-to-speech from the final text response
    let audioResponse: string | undefined = undefined;
    if (textResponse) {
      const { media } = await ai.generate({
        model: googleAI.model('gemini-2.5-flash-preview-tts'),
        config: {
          responseModalities: ['AUDIO'],
          speechConfig: {
            voiceConfig: {
              prebuiltVoiceConfig: { voiceName: 'Algenib' },
            },
          },
        },
        prompt: textResponse,
      });

      if (media?.url) {
        const audioBuffer = Buffer.from(media.url.substring(media.url.indexOf(',') + 1), 'base64');
        const wavBase64 = await toWav(audioBuffer);
        audioResponse = `data:audio/wav;base64,${wavBase64}`;
      }
    }

    return {
      textResponse,
      imageResponse,
      audioResponse,
    };
  }
);
