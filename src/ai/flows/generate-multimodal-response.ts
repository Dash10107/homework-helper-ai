'use server';
/**
 * @fileOverview A multimodal response generation AI agent named Sage.
 *
 * - generateMultimodalResponse - A function that handles the multimodal response generation process.
 * - GenerateMultimodalResponseInput - The input type for the generateMultimodalResponse function.
 * - GenerateMultimodalResponseOutput - The return type for the generateMultimodalResponse function.
 */

import { ai } from '@/ai/genkit';
import { z } from 'genkit';
import { googleAI } from '@genkit-ai/googleai';
import wav from 'wav';

// Define input schema
const GenerateMultimodalResponseInputSchema = z.object({
  questionText: z.string().optional().describe('The question asked by the student as text.'),
  questionImage: z
    .string()
    .optional()
    .describe(
      "A photo of a question, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
  questionAudio: z
    .string()
    .optional()
    .describe(
      "An audio recording of a question, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
});
export type GenerateMultimodalResponseInput = z.infer<typeof GenerateMultimodalResponseInputSchema>;

// Define output schema
const GenerateMultimodalResponseOutputSchema = z.object({
  textResponse: z.string().describe('The text part of the answer.'),
  imageResponse: z.string().optional().describe('An image generated by the AI, as a data URI.'),
  audioResponse: z.string().optional().describe('A spoken explanation, as a data URI.'),
});
export type GenerateMultimodalResponseOutput = z.infer<typeof GenerateMultimodalResponseOutputSchema>;


// The schema for the AI's reasoning process.
const ReasoningSchema = z.object({
  intent_summary: z.string().describe("A summary of the student's core question and intent."),
  response_type: z
    .enum(['text', 'image', 'audio', 'multimodal'])
    .describe('The best modality for the response (text, image, audio, or a combination).'),
  enhanced_prompt: z
    .string()
    .describe(
      'An enhanced, precise prompt for Gemini to generate the final answer, including any inferred context or subject tags.'
    ),
  image_prompt: z.string().optional().describe('The prompt for image generation, if needed.'),
  tts_text_prompt: z
    .string()
    .optional()
    .describe(
      'The text to be converted to speech. This should be the final explanation, formatted for being spoken naturally.'
    ),
});

// Exported function to be called from the frontend
export async function generateMultimodalResponse(
  input: GenerateMultimodalResponseInput
): Promise<GenerateMultimodalResponseOutput> {
  return generateMultimodalResponseFlow(input);
}

// Helper to convert PCM audio to WAV format
async function toWav(
  pcmData: Buffer,
  channels = 1,
  rate = 24000,
  sampleWidth = 2
): Promise<string> {
  return new Promise((resolve, reject) => {
    const writer = new wav.Writer({
      channels,
      sampleRate: rate,
      bitDepth: sampleWidth * 8,
    });
    const bufs: any[] = [];
    writer.on('error', reject);
    writer.on('data', d => bufs.push(d));
    writer.on('end', () => resolve(Buffer.concat(bufs).toString('base64')));
    writer.write(pcmData);
    writer.end();
  });
}

// Genkit flow definition
const generateMultimodalResponseFlow = ai.defineFlow(
  {
    name: 'generateMultimodalResponseFlow',
    inputSchema: GenerateMultimodalResponseInputSchema,
    outputSchema: GenerateMultimodalResponseOutputSchema,
  },
  async input => {
    // Step 1 & 2: Intent Understanding, Reasoning, and Prompt Enhancement
    const reasoningPrompt = `You are Sage, a multimodal educational assistant for students.
Your first task is to deeply understand the student's question, which may come in text, image, or audio format.
Analyze the input, infer any missing context, and determine the best way to respond.

Student's question:
- Text: "${input.questionText || 'N/A'}"
{{#if input.questionImage}}
- Image: {{media url=input.questionImage}}
{{/if}}
{{#if input.questionAudio}}
- Audio: {{media url=input.questionAudio}}
{{/if}}

Based on this, perform the following reasoning steps and return the result in JSON format:
1.  **intent_summary**: Briefly summarize the student's core question.
2.  **response_type**: Decide the most effective response modality ('text', 'image', 'audio', 'multimodal'). Use 'image' or 'multimodal' if the problem is visual (e.g., geometry, diagrams). Use 'audio' if the query was audio or a spoken explanation is best.
3.  **enhanced_prompt**: Rewrite the user's query into a clear, complete, and precise prompt for another AI to generate the final answer. Be student-friendly.
4.  **image_prompt**: If response_type is 'image' or 'multimodal', create a prompt to generate a helpful diagram, sketch, or visual. Otherwise, leave this null.
5.  **tts_text_prompt**: If response_type is 'audio' or 'multimodal', define the full text to be spoken. This will be the main explanation. Otherwise, leave this null.
`;

    const reasoningResponse = await ai.generate({
      prompt: reasoningPrompt.replace('{{media url=input.questionImage}}', input.questionImage ? `{{media url=${input.questionImage}}}` : '').replace('{{media url=input.questionAudio}}', input.questionAudio ? `{{media url=${input.questionAudio}}}` : ''),
      model: googleAI.model('gemini-2.0-flash'),
      output: { schema: ReasoningSchema },
    });
    
    const reasoningResult = reasoningResponse.output();
    if (!reasoningResult) {
      throw new Error('AI failed to reason about the request.');
    }

    // Step 3: Generate Response based on reasoning
    let textResponse = '';
    let imageResponse: string | undefined = undefined;
    let audioResponse: string | undefined = undefined;

    // Generate Text from enhanced prompt
    const finalAnswerResponse = await ai.generate({
        prompt: reasoningResult.enhanced_prompt,
        model: googleAI.model('gemini-2.0-flash'),
    });
    textResponse = finalAnswerResponse.text;

    // Generate Image if needed
    if (reasoningResult.image_prompt) {
      const { media } = await ai.generate({
        model: 'googleai/gemini-2.0-flash-preview-image-generation',
        prompt: reasoningResult.image_prompt,
        config: {
          responseModalities: ['TEXT', 'IMAGE'],
        },
      });
      imageResponse = media?.url;
    }
    
    // Generate Audio if needed
    const ttsContent = reasoningResult.tts_text_prompt || (reasoningResult.response_type === 'audio' ? textResponse : undefined);
    if (ttsContent) {
        const { media } = await ai.generate({
            model: googleAI.model('gemini-2.5-flash-preview-tts'),
            config: {
                responseModalities: ['AUDIO'],
                speechConfig: {
                    voiceConfig: {
                        prebuiltVoiceConfig: { voiceName: 'Algenib' },
                    },
                },
            },
            prompt: ttsContent,
        });

        if (media?.url) {
            const audioBuffer = Buffer.from(media.url.substring(media.url.indexOf(',') + 1), 'base64');
            const wavBase64 = await toWav(audioBuffer);
            audioResponse = `data:audio/wav;base64,${wavBase64}`;
        }
    }

    return {
      textResponse,
      imageResponse,
      audioResponse,
    };
  }
);
