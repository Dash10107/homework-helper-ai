'use server';
/**
 * @fileOverview A multimodal response generation AI agent named Sage.
 *
 * - generateMultimodalResponse - A function that handles the multimodal response generation process.
 * - GenerateMultimodalResponseInput - The input type for the generateMultimodalResponse function.
 * - GenerateMultimodalResponseOutput - The return type for the generateMultimodalResponse function.
 */

import { ai } from '@/ai/genkit';
import { z } from 'genkit';
import { googleAI } from '@genkit-ai/googleai';
import wav from 'wav';

// Define input schema
const GenerateMultimodalResponseInputSchema = z.object({
  questionText: z.string().optional().describe('The question asked by the student as text.'),
  questionImage: z
    .string()
    .optional()
    .describe(
      "A photo of a question, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
  questionAudio: z
    .string()
    .optional()
    .describe(
      "An audio recording of a question, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
  userPrefersAudioReply: z.boolean().optional().describe('Whether the user has requested an audio reply.'),
});
export type GenerateMultimodalResponseInput = z.infer<typeof GenerateMultimodalResponseInputSchema>;

// Define output schema
const GenerateMultimodalResponseOutputSchema = z.object({
  textResponse: z.string().describe('The text part of the answer.'),
  imageResponse: z.string().optional().describe('An image generated by the AI, as a data URI.'),
  audioResponse: z.string().optional().describe('A spoken explanation, as a data URI.'),
});
export type GenerateMultimodalResponseOutput = z.infer<typeof GenerateMultimodalResponseOutputSchema>;


// The schema for the AI's reasoning process.
const ReasoningSchema = z.object({
  response_type: z
    .enum(['text', 'image', 'audio', 'multimodal'])
    .describe('The best modality for the response (text, image, audio, or a combination).'),
  text: z.string().describe('The explanation in text form...'),
  tts_text: z.string().optional().describe('The same explanation rewritten for audio playback...'),
  image_prompt: z.string().optional().describe('If an image is needed, describe it for generation...'),
  confidence: z.number().min(0.0).max(1.0).describe('The confidence in the response from 0.0 to 1.0.'),
});

// Exported function to be called from the frontend
export async function generateMultimodalResponse(
  input: GenerateMultimodalResponseInput
): Promise<GenerateMultimodalResponseOutput> {
  return generateMultimodalResponseFlow(input);
}

// Helper to convert PCM audio to WAV format
async function toWav(
  pcmData: Buffer,
  channels = 1,
  rate = 24000,
  sampleWidth = 2
): Promise<string> {
  return new Promise((resolve, reject) => {
    const writer = new wav.Writer({
      channels,
      sampleRate: rate,
      bitDepth: sampleWidth * 8,
    });
    const bufs: any[] = [];
    writer.on('error', reject);
    writer.on('data', d => bufs.push(d));
    writer.on('end', () => resolve(Buffer.concat(bufs).toString('base64')));
    writer.write(pcmData);
    writer.end();
  });
}

// Genkit flow definition
const generateMultimodalResponseFlow = ai.defineFlow(
  {
    name: 'generateMultimodalResponseFlow',
    inputSchema: GenerateMultimodalResponseInputSchema,
    outputSchema: GenerateMultimodalResponseOutputSchema,
  },
  async input => {
    const reasoningPrompt = `You are Sage, a multimodal AI tutor helping students understand their homework questions. You accept queries in the form of text, image, or audio, and respond in the best modality (text, image, audio, or a combination). Your primary goal is to give clear, educational answers suited to the student's question and preferred response format.

---

ðŸŽ¯ Your Mission:

Step 1: Understand the Question
- Carefully analyze the userâ€™s input:
  - Text: "${input.questionText || 'N/A'}"
  {{#if input.questionImage}}
  - Image: {{media url=input.questionImage}}
  {{/if}}
  {{#if input.questionAudio}}
  - Audio: {{media url=input.questionAudio}}
  {{/if}}
- Determine the subject (e.g., math, science, history, coding).
- Infer the intent and fill in any missing context.
- Note the user preference for audio: \`user_prefers_audio_reply = ${!!input.userPrefersAudioReply}\`.

Step 2: Decide the Best Response Type
- Choose the response format based on both:
  a) The nature of the question (e.g., image if it's about a diagram),
  b) The userâ€™s preference (if they requested an audio reply).
- Modalities you can use:
  - Text explanation
  - Image (diagram, chart, or annotated visual)
  - Audio (spoken explanation via TTS)
  - Combination (e.g., image + audio + text)

Step 3: Generate Helpful Content
- If responding in audio, prepare clear spoken language suitable for Text-to-Speech (TTS).
- If generating an image, ensure it is labeled or self-explanatory.
- If generating text, use a helpful, conversational tone with bullet points or steps if needed.

Step 4: Return a Response Object in the specified JSON format.

---

ðŸ“Œ Guidelines:
- Be accurate and helpful â€” no hallucinations.
- Never guess if unsure. Politely ask for more details.
- Use simple language that a school student can understand.
- If user uploads audio: transcribe and treat it as a question.
- If user uploads image: use OCR or vision reasoning.
- If user prefers audio response, always include a "tts_text" field.
---

Now, perform your reasoning and provide the response object in JSON format.`;

    const reasoningResponse = await ai.generate({
      prompt: reasoningPrompt.replace('{{media url=input.questionImage}}', input.questionImage ? `{{media url=${input.questionImage}}}` : '').replace('{{media url=input.questionAudio}}', input.questionAudio ? `{{media url=${input.questionAudio}}}` : ''),
      model: googleAI.model('gemini-2.0-flash'),
      output: { schema: ReasoningSchema },
    });
    
    const reasoningResult = reasoningResponse.output;
    if (!reasoningResult) {
      throw new Error('AI failed to reason about the request.');
    }

    // Step 3: Generate Response based on reasoning
    let textResponse = reasoningResult.text;
    let imageResponse: string | undefined = undefined;
    let audioResponse: string | undefined = undefined;

    // Generate Image if needed
    if (reasoningResult.image_prompt) {
      const { media } = await ai.generate({
        model: 'googleai/gemini-2.0-flash-preview-image-generation',
        prompt: reasoningResult.image_prompt,
        config: {
          responseModalities: ['TEXT', 'IMAGE'],
        },
      });
      imageResponse = media?.url;
    }
    
    // Generate Audio if needed
    if (reasoningResult.tts_text) {
        const { media } = await ai.generate({
            model: googleAI.model('gemini-2.5-flash-preview-tts'),
            config: {
                responseModalities: ['AUDIO'],
                speechConfig: {
                    voiceConfig: {
                        prebuiltVoiceConfig: { voiceName: 'Algenib' },
                    },
                },
            },
            prompt: reasoningResult.tts_text,
        });

        if (media?.url) {
            const audioBuffer = Buffer.from(media.url.substring(media.url.indexOf(',') + 1), 'base64');
            const wavBase64 = await toWav(audioBuffer);
            audioResponse = `data:audio/wav;base64,${wavBase64}`;
        }
    }

    return {
      textResponse,
      imageResponse,
      audioResponse,
    };
  }
);
